{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "completeted data-training.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OqAER9e9obZS",
        "outputId": "28c9f470-ac55-47cb-ba4e-9028d3232d4f"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5yOmb-1Uo4AI",
        "outputId": "a1105ac0-c03e-41ed-e4f1-f5fdfa263723"
      },
      "source": [
        "!pip install keras --upgrade\n",
        "!pip install zipfile36\n",
        "!pip install pydicom"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: keras in /usr/local/lib/python3.7/dist-packages (2.8.0)\n",
            "Collecting zipfile36\n",
            "  Downloading zipfile36-0.1.3-py3-none-any.whl (20 kB)\n",
            "Installing collected packages: zipfile36\n",
            "Successfully installed zipfile36-0.1.3\n",
            "Collecting pydicom\n",
            "  Downloading pydicom-2.2.2-py3-none-any.whl (2.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.0 MB 2.6 MB/s \n",
            "\u001b[?25hInstalling collected packages: pydicom\n",
            "Successfully installed pydicom-2.2.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WMvotRMjo6nO"
      },
      "source": [
        "import keras\n",
        "import numpy as np\n",
        "import cv2\n",
        "import os\n",
        "import random\n",
        "import shutil\n",
        "import pandas as pd\n",
        "import csv\n",
        "import pydicom as dicom\n",
        "import zipfile\n",
        "from keras import optimizers\n",
        "from pathlib import Path\n",
        "from keras.models import Sequential,Model\n",
        "from keras.layers import Dropout, Flatten, Dense,Input\n",
        "from keras.applications.resnet_v2 import ResNet50V2\n",
        "from keras.applications.xception import Xception\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.applications.imagenet_utils import preprocess_input\n",
        "from keras import backend as K\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.initializers import RandomNormal\n",
        "from tensorflow.keras import optimizers"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aBej2ZX-qdPm",
        "outputId": "333ba305-0086-4d5d-e403-0b1650bc9b5a"
      },
      "source": [
        "!git clone https://github.com/ieee8023/covid-chestxray-dataset"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'covid-chestxray-dataset'...\n",
            "remote: Enumerating objects: 3641, done.\u001b[K\n",
            "remote: Total 3641 (delta 0), reused 0 (delta 0), pack-reused 3641\u001b[K\n",
            "Receiving objects: 100% (3641/3641), 632.96 MiB | 40.62 MiB/s, done.\n",
            "Resolving deltas: 100% (1450/1450), done.\n",
            "Checking out files: 100% (1174/1174), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PIZb_i1Oo9cj"
      },
      "source": [
        "archive = zipfile.ZipFile('/content/drive/MyDrive/Colab Notebooks/rsna-pneumonia-detection-challenge.zip') #Extract Kaggle Dataset\n",
        "for file in archive.namelist():\n",
        "     archive.extract(file, './All')"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0NnRLE9eo_sB"
      },
      "source": [
        "fold_num=1 "
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2n65GZwlpBYv"
      },
      "source": [
        "try:\n",
        "  os.mkdir('All/All')\n",
        "except:\n",
        "  pass"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SkKL3qDWpDJA"
      },
      "source": [
        "shutil.copy('/content/drive/MyDrive/Colab Notebooks/prepared_csv_files/All.csv','All')\n",
        "for i in range(1,9): \n",
        "  shutil.copy('/content/drive/MyDrive/Colab Notebooks/prepared_csv_files/fold{}/train{}.csv'.format(fold_num,i),'.')\n",
        "  globals()['train{}'.format(i)]=[]"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T4exV-czpGOJ"
      },
      "source": [
        "images=[]\n",
        "for r,d,f in os.walk('All/stage_2_train_images'):\n",
        "  for file in f:\n",
        "    images.append(os.path.join(r,file))\n",
        "for r,d,f in os.walk('covid-chestxray-dataset/images'):\n",
        "  for file in f:\n",
        "   images.append(os.path.join(r,file))"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ghqg7czUpH6i"
      },
      "source": [
        "csv_all=pd.read_csv('All/All.csv', nrows=None) #Read the CSV file that contains the names of the images with their labels.\n",
        "for index, row in csv_all.iterrows(): #This loop reads the images, converts them to suitable format and saves them in the All directory\n",
        "  if '.png' in row['filename']: #For creating the All.csv we have converted the kaggle dataset images to png format,\n",
        "                                #but some of the images in the other dataset also are in the format of png, so we use try/except here to distinguish which dataset, the annotation in the CSV file belongs to.\n",
        "    try:\n",
        "      png_index=row['filename'].find('.png')\n",
        "      last_name=row['filename'][:png_index]+'.dcm'\n",
        "      ds = dicom.dcmread(os.path.join('All/stage_2_train_images',last_name))\n",
        "      pixel_array_numpy = ds.pixel_array\n",
        "      imgname = last_name[:-4]+'.png'\n",
        "      cv2.imwrite(os.path.join('All/All', imgname), pixel_array_numpy)\n",
        "    except:\n",
        "      pass\n",
        "  else:\n",
        "    img=cv2.imread(os.path.join('covid-chestxray-dataset/images',row['filename']))\n",
        "    gray=cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
        "    cv2.imwrite(os.path.join('All/All', row['filename']), gray)  "
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ny4rjGTOpJeM"
      },
      "source": [
        "#optional and can be ignored\n",
        "All=[] \n",
        "all_train=[]\n",
        "all_test=[]\n",
        "with open('All/All.csv',newline='', mode='r') as csvfile:\n",
        "      csvreader = csv.reader(csvfile, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
        "      for row in csvreader:\n",
        "          All.append(row)\n",
        "for i in range(1,9): \n",
        "  with open('train{}.csv'.format(i),newline='', mode='r') as csvfile:\n",
        "      csvreader = csv.reader(csvfile, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
        "      for row in csvreader:\n",
        "        all_train.append(row)\n",
        "with open('all_test.csv'.format(i),newline='', mode='w') as csvfile: \n",
        "    csvwriter = csv.writer(csvfile, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
        "    csvwriter.writerow(['filename','class'])\n",
        "    for row in All:\n",
        "      if row not in all_train:\n",
        "        csvwriter.writerow(row)\n",
        "all_train=[] \n",
        "all_test=[]\n",
        "with open('All/All.csv',newline='', mode='r') as csvfile: \n",
        "      csvreader = csv.reader(csvfile, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
        "      for row in csvreader:\n",
        "          All.append(row)\n",
        "for i in range(1,9): \n",
        "  with open('train{}.csv'.format(i),newline='', mode='r') as csvfile:\n",
        "      csvreader = csv.reader(csvfile, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
        "      for row in csvreader:\n",
        "        all_train.append(row)\n",
        "with open('all_test.csv'.format(i),newline='', mode='w') as csvfile: \n",
        "    csvwriter = csv.writer(csvfile, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
        "    csvwriter.writerow(['filename','class'])\n",
        "    for row in All:\n",
        "      if row not in all_train:\n",
        "        csvwriter.writerow(row)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PKW4YIx3pL3V"
      },
      "source": [
        "for i in range(10): #Shuffle the All list\n",
        "  random.shuffle(All)\n",
        "with open('s_test.csv'.format(i),newline='', mode='w') as csvfile: #Create s_test.csv file for evaluating the network during training\n",
        "    csvwriter = csv.writer(csvfile, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
        "    csvwriter.writerow(['filename','class'])\n",
        "    ln=0\n",
        "    lp=0\n",
        "    for row in All:\n",
        "      if row not in all_train:\n",
        "        if row[1]=='COVID-19':\n",
        "          csvwriter.writerow(row)\n",
        "        elif row[1]=='normal':\n",
        "          if ln<300:\n",
        "            csvwriter.writerow(row)\n",
        "            ln+=1\n",
        "        else:\n",
        "          if lp<300:\n",
        "            csvwriter.writerow(row)\n",
        "            lp+=1"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JWb93QqopO1h"
      },
      "source": [
        "\n",
        "try:\n",
        "  os.remove('kaggle.zip')\n",
        "  shutil.rmtree('All/stage_2_train_images')\n",
        "  shutil.rmtree('All/stage_2_test_images')\n",
        "  shutil.rmtree('covid-chestxray-dataset')\n",
        "except:\n",
        "  pass"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fsT0qoQ-pQ2g"
      },
      "source": [
        "\n",
        "train_datagen = ImageDataGenerator(horizontal_flip=True,vertical_flip=True,rescale=1./255,zoom_range=0.05,rotation_range=360,width_shift_range=0.05,height_shift_range=0.05,shear_range=0.05)\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "test_df = pd.read_csv(\"s_test.csv\")"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qZz_U0nWpStI",
        "outputId": "669461ca-aa25-4645-d116-3ed929c55e3c"
      },
      "source": [
        "#training starts\n",
        "import keras.backend as k\n",
        "k.clear_session() \n",
        "try:\n",
        "  os.mkdir('models')\n",
        "except:\n",
        "  pass\n",
        "full_name='concatenate'\n",
        "classes_number=3 \n",
        "input_tensor=Input(shape=(300,300,3))\n",
        "######################################################################################################\n",
        "base_model1 = Xception(weights='imagenet', include_top=False, input_tensor=input_tensor)\n",
        "features1 = base_model1.output\n",
        "######################################################################################################\n",
        "base_model2 = ResNet50V2(weights='imagenet', include_top=False, input_tensor=input_tensor)\n",
        "features2 = base_model2.output\n",
        "concatenated=keras.layers.concatenate([features1,features2])\n",
        "####################################################################################################\n",
        "conv=keras.layers.Conv2D(1024, (1, 1),padding='same')(concatenated) \n",
        "feature = Flatten(name='flatten')(conv)\n",
        "dp = Dropout(0.5)(feature)\n",
        "preds = Dense(classes_number, activation='softmax', kernel_initializer=RandomNormal(mean=0.0, stddev=0.001))(dp) \n",
        "Concatenated_model = Model(inputs=input_tensor, outputs=preds)\n",
        "#######################################################\n",
        "for layer in Concatenated_model.layers:\n",
        "  layer.trainable = True\n",
        "Concatenated_model.compile(optimizer=optimizers.Adam(lr=0.0001), loss='categorical_crossentropy',metrics=['accuracy'])\n",
        "filepath=\"models/%s-{epoch:02d}-{val_accuracy:.4f}.hdf5\"%full_name \n",
        "checkpoint = ModelCheckpoint(filepath, monitor='val_accuracy', save_best_only=True, mode='max') #creating checkpoint \n",
        "callbacks_list = [checkpoint]\n",
        "\n",
        "for epoch_num in range(1,9): \n",
        "  train_df =pd.read_csv(\"train{}.csv\".format(epoch_num)) \n",
        "  train_generator = train_datagen.flow_from_dataframe(\n",
        "        dataframe=train_df,\n",
        "        directory='All/All',\n",
        "        x_col=\"filename\",\n",
        "        y_col=\"class\",\n",
        "        target_size=(300, 300),\n",
        "        batch_size=20,\n",
        "        class_mode='categorical',shuffle=True)\n",
        "  validation_generator = test_datagen.flow_from_dataframe(\n",
        "          dataframe=test_df,\n",
        "          directory='All/All',\n",
        "          x_col=\"filename\",\n",
        "          y_col=\"class\",\n",
        "          target_size=(300, 300),\n",
        "          batch_size=20,\n",
        "          class_mode='categorical',shuffle=True)\n",
        "Concatenated_model.fit_generator(train_generator,validation_data = validation_generator, epochs=5,shuffle=True) #start training"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/xception/xception_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "83689472/83683744 [==============================] - 0s 0us/step\n",
            "83697664/83683744 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50v2_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "94674944/94668760 [==============================] - 1s 0us/step\n",
            "94683136/94668760 [==============================] - 1s 0us/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n",
            "/usr/local/lib/python3.7/dist-packages/keras_preprocessing/image/dataframe_iterator.py:282: UserWarning: Found 39 invalid image filename(s) in x_col=\"filename\". These filename(s) will be ignored.\n",
            "  .format(n_invalid, x_col)\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:49: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 594 validated image filenames belonging to 3 classes.\n",
            "Found 662 validated image filenames belonging to 3 classes.\n",
            "Found 594 validated image filenames belonging to 3 classes.\n",
            "Found 662 validated image filenames belonging to 3 classes.\n",
            "Found 594 validated image filenames belonging to 3 classes.\n",
            "Found 662 validated image filenames belonging to 3 classes.\n",
            "Found 594 validated image filenames belonging to 3 classes.\n",
            "Found 662 validated image filenames belonging to 3 classes.\n",
            "Found 594 validated image filenames belonging to 3 classes.\n",
            "Found 662 validated image filenames belonging to 3 classes.\n",
            "Found 594 validated image filenames belonging to 3 classes.\n",
            "Found 662 validated image filenames belonging to 3 classes.\n",
            "Found 594 validated image filenames belonging to 3 classes.\n",
            "Found 662 validated image filenames belonging to 3 classes.\n",
            "Found 594 validated image filenames belonging to 3 classes.\n",
            "Found 662 validated image filenames belonging to 3 classes.\n",
            "Epoch 1/5\n",
            "30/30 [==============================] - 72s 2s/step - loss: 0.8047 - accuracy: 0.6768 - val_loss: 1.0560 - val_accuracy: 0.6556\n",
            "Epoch 2/5\n",
            "30/30 [==============================] - 40s 1s/step - loss: 0.7430 - accuracy: 0.7424 - val_loss: 0.6882 - val_accuracy: 0.7402\n",
            "Epoch 3/5\n",
            "30/30 [==============================] - 40s 1s/step - loss: 0.5796 - accuracy: 0.7946 - val_loss: 0.6408 - val_accuracy: 0.7749\n",
            "Epoch 4/5\n",
            "30/30 [==============================] - 41s 1s/step - loss: 0.6051 - accuracy: 0.8047 - val_loss: 1.0613 - val_accuracy: 0.6798\n",
            "Epoch 5/5\n",
            "30/30 [==============================] - 41s 1s/step - loss: 0.5622 - accuracy: 0.7845 - val_loss: 0.5599 - val_accuracy: 0.8112\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fd8d1af3450>"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JbkdhdN42K9J",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ae3a33c8-e29f-4e25-92dc-97b8b75eb9b8"
      },
      "source": [
        "#saving model\n",
        "import pickle\n",
        "with open('saved-model','wb') as f:\n",
        "  pickle.dump(Concatenated_model,f)\n",
        "\n",
        "# load model\n",
        "# with open('saved-model','rb') as f:\n",
        "#   loaded_model = pickle.load(f)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Assets written to: ram://e7f0136c-d6a6-411a-a6e1-e2f34a4a6270/assets\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Y_ZXxPypWS1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f1b73bd2-56f6-4625-a96f-d1caebef5eeb"
      },
      "source": [
        "trained_models=[]\n",
        "for r,d,f in os.walk('models'):\n",
        "  for file in f:\n",
        "    trained_models.append(os.path.join(r,file)) \n",
        "reports={}\n",
        "test_df = pd.read_csv(\"all_test.csv\") #Load the full test CSV file that includes 11302 images\n",
        "validation_generator = test_datagen.flow_from_dataframe( \n",
        "        dataframe=test_df,\n",
        "        directory='All/All',\n",
        "        x_col=\"filename\",\n",
        "        y_col=\"class\",\n",
        "        target_size=(300, 300),\n",
        "        batch_size=20,\n",
        "        class_mode='categorical',shuffle=True)\n",
        "for trained_model in trained_models:\n",
        "  k.clear_session()\n",
        "  net=keras.models.load_model(trained_model) #load model\n",
        "  covid_label= validation_generator.class_indices['COVID-19'] \n",
        "  pneu_label= validation_generator.class_indices['pneumonia']  \n",
        "  normal_label= validation_generator.class_indices['normal']  \n",
        "  tp=0 #True Positives\n",
        "  fp=0 #False Positives\n",
        "  anum=0 #All the images numbers\n",
        "  ###########\n",
        "  wrong_covid=0\n",
        "  correct_covid=0\n",
        "  not_detected_covid=0\n",
        "  covid_num=0\n",
        "  ###########\n",
        "  wrong_pneu=0\n",
        "  correct_pneu=0\n",
        "  not_detected_pneu=0\n",
        "  pneu_num=0\n",
        "  ############\n",
        "  wrong_normal=0\n",
        "  correct_normal=0\n",
        "  not_detected_normal=0\n",
        "  normal_num=0\n",
        "  ##############\n",
        "  wrong_covid_normal=0 \n",
        "  wrong_covid_pneu=0   \n",
        "  wrong_pneu_covid=0   \n",
        "  wrong_pneu_normal=0 \n",
        "  wrong_normal_pneu=0  \n",
        "  wrong_normal_covid=0  \n",
        "  ################\n",
        "  for num,img_name in enumerate(validation_generator.filenames): #load image\n",
        "    gt_ind=validation_generator.classes[num] #get the loaded image class index\n",
        "    img=cv2.resize(cv2.imread(os.path.join('All','All',img_name)),(300,300)) #resize image\n",
        "    img=img.astype('float32') / 255.0 #scale the image\n",
        "    pred_ind=np.argmax(net.predict(np.expand_dims(img,axis=0))[0]) #get the predicted class index\n",
        "\n",
        "    anum+=1 #count the number of images\n",
        "\n",
        "\n",
        "    if gt_ind==covid_label:\n",
        "      covid_num+=1\n",
        "    if gt_ind==pneu_label:\n",
        "      pneu_num+=1\n",
        "    if gt_ind==normal_label:\n",
        "      normal_num+=1\n",
        "    ##################\n",
        "    if gt_ind==covid_label and pred_ind==covid_label: \n",
        "      correct_covid+=1\n",
        "    if gt_ind==covid_label and pred_ind!=covid_label:\n",
        "      not_detected_covid+=1\n",
        "      if pred_ind==pneu_label:\n",
        "        wrong_covid_pneu+=1\n",
        "      elif pred_ind==normal_label:\n",
        "        wrong_covid_normal+=1\n",
        "    if gt_ind!=covid_label and pred_ind==covid_label:\n",
        "      wrong_covid+=1\n",
        "    ###########################################\n",
        "    if gt_ind==normal_label and pred_ind==normal_label: \n",
        "      correct_normal+=1\n",
        "    if gt_ind==normal_label and pred_ind!=normal_label:\n",
        "      not_detected_normal+=1\n",
        "      if pred_ind==pneu_label:\n",
        "        wrong_normal_pneu+=1\n",
        "      elif pred_ind==covid_label:\n",
        "        wrong_normal_covid+=1\n",
        "    if gt_ind!=normal_label and pred_ind==normal_label:\n",
        "      wrong_normal+=1\n",
        "    ###########################################\n",
        "    if gt_ind==pneu_label and pred_ind==pneu_label: \n",
        "      correct_pneu+=1\n",
        "    if gt_ind==pneu_label and pred_ind!=pneu_label:\n",
        "      not_detected_pneu+=1\n",
        "      if pred_ind==normal_label:\n",
        "        wrong_pneu_normal+=1\n",
        "      elif pred_ind==covid_label:\n",
        "        wrong_pneu_covid+=1\n",
        "    if gt_ind!=pneu_label and pred_ind==pneu_label:\n",
        "      wrong_pneu+=1\n",
        "    ###########################################\n",
        "\n",
        "    if pred_ind==gt_ind:\n",
        "      tp+=1\n",
        "    else:\n",
        "      fp+=1"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 22604 validated image filenames belonging to 3 classes.\n"
          ]
        }
      ]
    }
  ]
}